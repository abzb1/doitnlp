{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ratsgo.github.io/nlpbook/   \n",
    "Do it!\n",
    "BERT와 GPT로 배우는 자연어 처리    \n",
    "트랜스포머 핵심원리와 허깅페이스 패키지 활용법   \n",
    "이기창 지음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화(Tokenization)란 문장을 토큰 시퀀스로 나누는 과정임   \n",
    "수행대상에 따라 문자, 단어, 서브워드 등 세가지 방법이 있음   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화를 수행하는 프로그램을 토크나이저(Tokenizer)라고 함   \n",
    "대표적인 한국어 토크나이저에는 은전한닢(mecab), 꼬꼬마(kkma)등이 있음   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어(어절) 단위 토큰화   \n",
    "   \n",
    "공백으로 분리하면 별도 토크나이저가 필요없지만 활용을 모두 포함해야하므로 어휘집합의 크기가 매우 커질 수 있음   \n",
    "   \n",
    "의미있는 단위로 토큰화하면 어휘집합이 급격하게 커지는 것을 막을 수 있음   \n",
    "\n",
    "어제 카페 갔었어   \n",
    "공백 분리: 어제, 카페, 갔었어   \n",
    "의미 있는 단위 분리: 어제, 카페, 갔었, 어   \n",
    "단어단위 토크나이저를 써도 어휘집합은 수만개, 수십만개이고 어휘집합이 커지면 학습이 어려워질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 단위 토큰화   \n",
    "   \n",
    "토큰화를 단어대신 문자 단위로 고려하면 모든 문자를 어휘집합에 포함시켜도 어휘집합이 작고 미등록 토큰(unknown token)의 문제로부터 자유로움   \n",
    "   \n",
    "그러나 문자 토큰이 의미를 갖긴 힘들고 token sequence가 길어져 학습하기 어려워지며, 결과적으로 성능이 떨어지게 됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서브워드 단위 토큰화   \n",
    "   \n",
    "서브워드(subword) 단위 토큰화는 단어와 문자 단위 토큰화의 중간에 있는 형태로 어휘 집합을 너무 키우지 않으면서, 미등록 토큰 문제를 피하고 분석된 토큰 시퀀스가 너무 길어지지 않게 함.   \n",
    "대표적인 서브워드 단위 토큰화에 바이트 페어 인코딩이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바이트 페어 인코딩(Byte Pair Encoding BPE)   \n",
    "   \n",
    "바이트 페어 인코딩은 원래 정보압축 알고리즘으로 제안됨   \n",
    "최근에는 자연어 처리 모델에 널리 쓰이는 토큰화 기법임   \n",
    "GPT가 BPE 기법으로 토큰화를 수행하며, BERT모델은 BPE와 유사한 워드피스(wordpiece)를 토크나이저로 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE란 무엇일까?   \n",
    "   \n",
    "1994년 제안된 정보 압축 알고리즘으로 데이터에서 가장 많이 등장한 문자열을 병합해서 데이터를 압축하는 기법\n",
    "\n",
    "예   \n",
    "aaabdaaabac라는 문자열이 있다고 가정   \n",
    "BPE는 데이터에 등장한 글자(a,b,c,d)를 초기 사전으로 구성하며, 연속된 두 글자를 한 글자로 병합, 가장 많이 등장한 aa를 Z로 병합하면 문자열을 다음과 같이 압축할 수 있음   \n",
    "aaabdaaabac -> ZabdZabac   \n",
    "다음으로 많이 등장한 ab를 Y로 병합\n",
    "ZabdZabac -> ZYdZYac\n",
    "ZY를 X로 병합\n",
    "XdXac\n",
    "사전은 (a,b,c,d,Z,Y,X)로 적당히 늘고, 데이터 길이는 11에서 5로 감소\n",
    "BPE는 사전의 크기를 지나치게 늘리지 않으면서 데이터 길이를 효율적으로 압축할 수 있는 알고리즘   \n",
    "   \n",
    "BPE기반 토큰화 기법은 분석 대상 언어에 대한 지식이 필요없음   \n",
    "말뭉치에서 자주 나타나는 문자열(서브워드)을 토큰으로 분석하기 때문   \n",
    "\n",
    "기계번역분야에서 자연어처리중 처음 BPE 사용   \n",
    "\n",
    "BPE를 활용한 토큰화 절차는 다음과 같음   \n",
    "1. 어휘 집합 구축: 자주 등장하는 문자열을 병합하고 이를 어취 집합에 추가합니다. 이를 원하는 어취 집합 크기가 될 때까지 반복합니다.   \n",
    "2. 토큰화: 토큰화 대상 문장의 각 어절에서 어휘 집합에 있는 서브워드가 포함되었을 때 해당 서브워드를 어절에서 분리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE 어휘집합 구축하기   \n",
    "   \n",
    "예시와 함께보는 BPE 어휘 집합을 구축하는 절차   \n",
    "\n",
    "1. 말뭉치 준비   \n",
    "   \n",
    "2. 말뭉치의 모든 문장을 공백으로 나누어줌->프로토크나이즈(pre-tokenize)   \n",
    "    공백외에 다른 방법으로도 프리토크나이즈 가능\n",
    "\n",
    "3. 초기 어휘 집합 설정\n",
    "    문자단위로 BPE 수행 시 초기어휘 집합: b, g, h, n, p, s, u\n",
    "\n",
    "4. 다음 표와 같이 프로토크나이즈 결과를 초기어휘집합을 사용한 빈도로 정리   \n",
    "    프리토큰을 이용해 만든 빈도표   \n",
    "    ![bpe_freq_1](images/BPE_freq_1.PNG)   \n",
    "    초기 어휘 집합을 이용해 만든 빈도표   \n",
    "    ![bpe_freq_2](images/BPE_freq_initial2.png)\n",
    "\n",
    "5. 빈도표를 바이그램으로 묶어 재정리   \n",
    "    바이그램으로 묶어 만든 빈도표   \n",
    "    ![bpe_freq_3](images/BPE_freq_bigram_1.PNG)   \n",
    "\n",
    "6. 같은 바이그램을 묶어 빈도표 재정리   \n",
    "    같은 바이그램으로 묶어 재정리한 빈도표   \n",
    "    ![bpe_freq_4](images/BPE_freq_bigram_2.PNG)   \n",
    "\n",
    "7. 가장 많이 등장한 바이그램을 하나로 묶고 어휘에 추가   \n",
    "    u,g를 ug로 합치고 추가 후의 어휘집합: b, g, h, n, p, s, u, ug   \n",
    "\n",
    "8. 4번의 결과를 새로운 어휘집합을 사용해서 재정리   \n",
    "    새로운 어휘집합으로 재정리한 빈도표   \n",
    "    ![bpe_freq_5](images/BPE_freq_new_vocab_1.PNG)   \n",
    "\n",
    "9. 8의 결과를 5, 6번으로 재정리   \n",
    "    바이그램으로 묶어 재정리한 빈도표   \n",
    "    ![bpe_freq_5](images/BPE_freq_new_vocab_2.PNG)   \n",
    "\n",
    "10. 9의 결과에서 가장 많이 등장한 바이그램을 다시 하나고 묶어 어휘집합에 추가\n",
    "    u, n을 un으로 묶은 결과\n",
    "    ![bpe_freq_5](images/BPE_freq_new_new_vocab_1.PNG)   \n",
    "\n",
    "11. 어휘집합이 원하는 크기가 될 때까지 반복   \n",
    "   \n",
    "\n",
    "정리하면\n",
    "BPE 어휘집합은 고빈도 바이그램 쌍을 병합하는 방식으로 구축한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병합과정의 병합 우선순위를 저장   \n",
    "1. u g   \n",
    "2. u n   \n",
    "3. h ug   \n",
    "...   \n",
    "   \n",
    "이 저장값은 BPE 토큰화 과정에서 서브워드 병합 우선순위를 정하는데 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE 토큰화   \n",
    "   \n",
    "어휘 집합과 병합 우선순위가 결정되면 토큰화를 수행할 수 있음\n",
    "예를 들어 pug bug mug라는 문장을 토큰화한다고하면\n",
    "1. 프리토크나이즈 수행   \n",
    "   공백단위로 분리   \n",
    "   pug, bug, mug   \n",
    "\n",
    "2. 첫번째 프리토큰을 문자단위로 분리   \n",
    "   p, u, g   \n",
    "\n",
    "3. 병합 우선 순위를 이용해 병합 우선순위를 부여   \n",
    "   p, u -> 우선순위에 없음   \n",
    "   u, g -> 1순위\n",
    "\n",
    "4. 높은 우선순위를 합침   \n",
    "   p, ug   \n",
    "\n",
    "5. 병합 우선 순위를 이용해 다시 확인   \n",
    "   p, ug -> 우선 순위에 없음   \n",
    "\n",
    "6. 병합 우선순위에 없으면 병합을 그만 둠   \n",
    "   \n",
    "7. 병합된 결과가 어휘집합에 있는지 확인   \n",
    "   p, ug 모두 있으므로 최종 토큰화 결과는 p, ug\n",
    "   m, u, g의 경우   \n",
    "   m, ug에서 m이 어휘집합에 없으므로 m은 미등록 토큰(unknown token) \\<unk\\>으로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드피스   \n",
    "   \n",
    "워드피스(wordpiece)는 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사   \n",
    "그러나 어휘 집합 구축 시 문자열 병합기준이 다름   \n",
    "BPE는 빈도를 기준으로 병합하지만, 워드피스는 말뭉치의 우도를 가장 높이는 쌍을 병합함   \n",
    "병합 후보 a, b에 대해서 우도란 a, b가 각각 등장할 확률의 곱을 분모로, ab가 연이어 등장할 확률을 분자로 하여 계산   \n",
    "### $\\frac{\\frac{\\#ab}{n}}{\\frac{\\#a}{n} \\times \\frac{\\#b}{n}}$   \n",
    "즉, ab가 a, b에 독립인 경우보다 더 자주 등장하는 경우에 병합순위가 높아짐   \n",
    "\n",
    "BPE는 토큰화에 어휘 집합과 병합우선순위가 둘 다 필요하지만\n",
    "워드피스는 토큰화에 어휘집합만 사용함   \n",
    "분석 대상 어절에 어휘 집합에 있는 서브워드가 포함된 경우 해당 서브워드를 어절에서 분리함   \n",
    "서브워드 후보가 여럿있는 경우 가장 긴 서브워드를 선택하며 반복   \n",
    "문자열에 서브워드 후보가 하나도 없으면 문자열 전체를 미등록 단어로 취급\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어휘 집합 구축 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의존성 패키지 설치\n",
    "# !pip install ratsnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:03, 4.74MB/s]                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 12.8MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "# 말뭉치 내려받기 및 전처리\n",
    "# 오픈소스 패키지 Korpora를 활용해 BPE 수행 대상 말뭉치 다운로드\n",
    "# 네이버 영화 리뷰 NSMC\n",
    "from Korpora import Korpora\n",
    "nsmc=Korpora.load(\"nsmc\", force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc 데이터 전처리\n",
    "import os\n",
    "\n",
    "def write_lines(path, lines):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "write_lines(\"./nsmc/train.txt\", nsmc.train.get_all_texts())\n",
    "write_lines(\"./nsmc/test.txt\", nsmc.test.get_all_texts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE 토크나이저 구축   \n",
    "GPT계열 모델이 사용하는 토크나이저 시법은 BPE   \n",
    "실제로는 예시의 문자 수준이 아닌, 유니코드 바이트 수준으로 어휘집합을 구축하고 토큰화를 수행함   \n",
    "유니코드 바이트 기준으로 BPE를 사용하면 미등록 토큰 문제에서 비교적 자유로울 수 있음   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./bbpe/vocab.json', './bbpe/merges.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 바이트 수준 BPE 어휘집합 구축\n",
    "# 시간이 조금 걸림(수십초)\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
    "bytebpe_tokenizer.train(\n",
    "    files=[\"./nsmc/train.txt\", \"./nsmc/test.txt\"], # 학습 말뭉치를 리스트로 전달\n",
    "    vocab_size=10000, # 어휘 집합 크기 조절\n",
    "    special_tokens=[\"[PAD]\"] # 특수 토큰 추가\n",
    ")\n",
    "bytebpe_tokenizer.save_model(\"./bbpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 토크나이저를 사용해 bbpe 경로에 BPE의 어휘 집합 vocab.json 파일과 병합 우선순위 merges.txt가 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드피스 토크나이저 구축   \n",
    "   \n",
    "BERT 는 워드피스 토크나이저를 사용함   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./wordpiece/vocab.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 워드피스 어휘집합 구축\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "wordpiece_tokenizer.train(\n",
    "    files=[\"./nsmc/train.txt\",\"./nsmc/test.txt\"],\n",
    "    vocab_size=10000,\n",
    ")\n",
    "wordpiece_tokenizer.save_model(\"./wordpiece\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 토크나이저를 사용해 wordpiece 경로에 wordpiece 어휘집합 vocab.txt가 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화하기   \n",
    "   \n",
    "생성한 BPE 어휘 집합과 병합 우선 순위 파일을 이용해서 GPT모델이 사용할 토크나이저를 초기화   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer_gpt=GPT2Tokenizer.from_pretrained(\"./bbpe\")\n",
    "tokenizer_gpt.pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ìķĦ', 'ĠëįĶë¹Ļ', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
       " ['íĿł',\n",
       "  '...',\n",
       "  'íı¬ìĬ¤íĦ°',\n",
       "  'ë³´ê³ł',\n",
       "  'Ġì´ĪëĶ©',\n",
       "  'ìĺģíĻĶ',\n",
       "  'ì¤Ħ',\n",
       "  '....',\n",
       "  'ìĺ¤ë²Ħ',\n",
       "  'ìĹ°ê¸°',\n",
       "  'ì¡°ì°¨',\n",
       "  'Ġê°Ģë³į',\n",
       "  'ì§Ģ',\n",
       "  'ĠìķĬ',\n",
       "  'êµ¬ëĤĺ'],\n",
       " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시문장 토큰화\n",
    "\n",
    "sentences = [\n",
    "    \"아 더빙 진짜 짜증나네요 목소리\",\n",
    "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
    "    \"별루 였다..\",\n",
    "]\n",
    "\n",
    "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 모델 입력 만들기\n",
    "batch_inputs = tokenizer_gpt(\n",
    "    sentences,\n",
    "    padding=\"max_length\",\n",
    "    max_length=12,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[334, 2338, 581, 4055, 464, 3808, 0, 0, 0, 0, 0, 0],\n",
       " [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292],\n",
       " [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰을 어휘집합에 인덱싱한 결과\n",
    "batch_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PAD] 토큰의 인덱스는 0   \n",
    "토큰 수가 12로 맞춰짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad 토큰에 대한 마스크\n",
    "batch_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 wordpiece 어휘집합을 사용하여 BERT용 토크나이저 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer_bert=BertTokenizer.from_pretrained(\n",
    "    \"./wordpiece\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['아', '더빙', '진짜', '짜증나', '##네요', '목소리'],\n",
       " ['흠',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '포스터',\n",
       "  '##보고',\n",
       "  '초딩',\n",
       "  '##영화',\n",
       "  '##줄',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '오버',\n",
       "  '##연기',\n",
       "  '##조차',\n",
       "  '가볍',\n",
       "  '##지',\n",
       "  '않',\n",
       "  '##구나'],\n",
       " ['별루', '였다', '.', '.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences=[tokenizer_bert.tokenize(sentence) for sentence in sentences]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##은 해당 토큰이 어절의 시작이 아님을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 621, 2631, 1993, 3678, 1990, 3323, 3, 0, 0, 0, 0],\n",
       " [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1138, 16, 3],\n",
       " [2, 3274, 9507, 16, 16, 3, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  BERT 모델 입력 만들기\n",
    "\n",
    "batch_inputs=tokenizer_bert(\n",
    "    sentences,\n",
    "    padding=\"max_length\",\n",
    "    max_length=12,\n",
    "    truncation=True,\n",
    ")\n",
    "batch_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번은 문장의 시작인 [CLS], 3번은 문장의 끝인 [SEP] 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩 마스크\n",
    "\n",
    "batch_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세그먼트(segment) 인덱스\n",
    "\n",
    "batch_inputs[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시퀀스의 토큰이 몇번째 세그먼트(문서 혹은 문장)로 부터 온 것인지 나타내는 인덱스   \n",
    "우리는 모두 한 문장이므로 0번"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퀴즈   \n",
    "1. 토큰화란 문장을 토큰 시퀀스로 나누는 것이다.   \n",
    "2. 토큰화 수행대상에 따라 문자 단위, 단어 단위, 서브워드 단위 세가지로 나뉜다.   \n",
    "3. 바이트 페어 인코딩은 사전 크기 증가를 억제하면서도 정보 압축이 가능한 알고리즘이다.   \n",
    "4. BPE 어휘집합은 고빈도 바이그램 쌍을 병합하는 방식으로 구축한다.   \n",
    "5. 워드피스는 우도를 가장 높이는 쌍을 병합한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
